{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408817a1",
   "metadata": {},
   "source": [
    "# Web Scraping con Python\n",
    "\n",
    "Profesor: Steven Van Vaerenbergh  \n",
    "28 de junio de 2023  \n",
    "Curso CA.2.1 Python para el análisis de datos  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfe0a0",
   "metadata": {},
   "source": [
    "## Índice\n",
    "\n",
    "1. [Introducción](#1.-Introducción)\n",
    "   1. [Aviso Inicial](#1.A.-Aviso-Inicial)\n",
    "   2. [¿Qué es el Web Scraping?](#1.B.-¿Qué-es-Web-Scraping?)\n",
    "   3. [Herramientas de Web Scraping en Python](#1.C.-Herramientas-y-librerías)\n",
    "2. [Conceptos de HTML](#2.-Conceptos-de-HTML)\n",
    "   1. [Estructura básica de una página web](#2.A.-Estructura-básica-de-una-página-web)\n",
    "   2. [Elementos comunes de HTML (etiquetas, atributos)](#2.B.-Elementos-comunes-de-HTML-(etiquetas,-atributos))\n",
    "   3. [Herramientas de inspección del navegador](#2.C.-Herramientas-de-inspección-del-navegador)\n",
    "3. [Usando la librería requests](#3.-Usando-la-librería-requests)\n",
    "   1. [Instalación y primeros pasos](#3.A.-Instalación-y-primeros-pasos)\n",
    "   2. [Realizando solicitudes HTTP (GET)](#3.B.-Realizando-solicitudes-HTTP-(GET))\n",
    "   3. [Explorando la respuesta (status code, content)](#3.C.-Explorando-la-respuesta-(status-code,-content))\n",
    "4. [Extracción de datos con Beautiful Soup](#4.-Extracción-de-datos-con-Beautiful-Soup)\n",
    "   1. [Instalación y primeros pasos](#4.A.-Instalación-y-primeros-pasos)\n",
    "   2. [Parseando HTML con Beautiful Soup](#4.B.-Parseando-HTML-con-Beautiful-Soup)\n",
    "   3. [Buscar y extraer elementos de la página](#4.C.-Buscar-y-extraer-elementos-de-la-página)\n",
    "5. [Ejercicio Práctico](#5.-Ejercicio-Práctico)\n",
    "   1. [Tareas Opcionales](#5.A.-Tareas-Opcionales)\n",
    "6. [Selenium](#6.-Selenium)\n",
    "   1. [¿Qué es Selenium?](#6.A.-¿Qué-es-Selenium?)\n",
    "   2. [¿Cuándo usar Selenium?](#6.B.-¿Cuándo-usar-Selenium?)\n",
    "7. [Recapitulación y Preguntas](#7.-Recapitulación-y-Preguntas)\n",
    "   1. [Resumen de lo aprendido](#7.A.-Resumen-de-lo-aprendido)\n",
    "   2. [Tiempo para preguntas](#7.B.-Tiempo-para-preguntas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3548ed7",
   "metadata": {},
   "source": [
    "## 1. Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030d75d",
   "metadata": {},
   "source": [
    "### 1.A. Aviso Inicial\n",
    "\n",
    "Antes de empezar, es importante mencionar que el web scraping debe hacerse de manera responsable y ética. No todas las páginas web permiten el web scraping. Algunos sitios web tienen términos de servicio que prohíben explícitamente esta práctica. Además, el web scraping intensivo puede sobrecargar los servidores de una página web, lo que puede afectar su rendimiento para otros usuarios. Cabe mencionar que muchas páginas web ofrecen una API que se puede usar para extraer datos de una manera más controlada y eficiente. Si no hay API disponible o no proporciona los datos que necesitamos, podemos considerar el web scraping. En cualquier caso, siempre debemos comprobar los términos de servicio de una página web antes de realizar web scraping y respetar las reglas del archivo `robots.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc2b8d3",
   "metadata": {},
   "source": [
    "### 1.B. ¿Qué es Web Scraping?\n",
    "\n",
    "El web scraping (\"extracción web\") es una técnica utilizada para extraer información de páginas web. Consiste en hacer una solicitud HTTP a la URL de una página web para obtener el código HTML de la página, y luego extraer los datos útiles de ese código HTML. El web scraping se utiliza en una variedad de contextos digitales, como la extracción de datos para machine learning, la recopilación de información para investigación de mercados, la monitorización de precios para comparación de precios, entre otros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f400de1",
   "metadata": {},
   "source": [
    "### 1.C. Herramientas y librerías\n",
    "\n",
    "Para hacer web scraping con Python, se utilizan varias librerías. En esta clase, nos centraremos en las librerías `requests` y `Beautiful Soup`:\n",
    "\n",
    "- **Requests**: es una librería de Python para enviar solicitudes HTTP. Es decir, nos permite solicitar el código HTML de una página web.\n",
    "\n",
    "- **Beautiful Soup**: es una librería de Python para parsear documentos HTML y XML. Nos permite navegar, buscar y modificar el árbol de sintaxis analizada.\n",
    "\n",
    "Más adelante en la clase, también mencionaremos brevemente `Selenium`, que es una herramienta para automatizar navegadores web. Es útil para los casos en los que la información que queremos extraer está contenida en JavaScript, que no puede ser accedido con `requests`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d736160",
   "metadata": {},
   "source": [
    "## 2. Conceptos de HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347d638",
   "metadata": {},
   "source": [
    "### 2.A. Estructura básica de una página web\n",
    "\n",
    "Una página web se estructura a través de un lenguaje de marcado llamado HTML (Hypertext Markup Language). HTML utiliza etiquetas para definir los elementos en la página. Cada página web tiene una estructura básica que incluye las siguientes partes:\n",
    "\n",
    "- `<!DOCTYPE html>`: Define el tipo de documento y la versión de HTML.\n",
    "- `<html>`: Es el elemento raíz de una página web.\n",
    "- `<head>`: Contiene metadatos/información sobre el documento.\n",
    "- `<title>`: Define el título del documento.\n",
    "- `<body>`: Contiene el contenido visible de la página web.\n",
    "\n",
    "Un ejemplo de la estructura básica de una página web es el siguiente:\n",
    "\n",
    "````\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Título de la página</title>\n",
    "</head>\n",
    "<body>\n",
    "    Contenido de la página\n",
    "</body>\n",
    "</html>\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b5b82",
   "metadata": {},
   "source": [
    "### 2.B. Elementos comunes de HTML (etiquetas, atributos)\n",
    "\n",
    "El HTML se compone de varios elementos definidos por etiquetas. Cada elemento de HTML puede tener atributos que definen características adicionales del elemento. Aquí se muestran algunos de los elementos más comunes:\n",
    "\n",
    "- `<div>`: Define una sección en un documento.\n",
    "- `<p>`: Define un párrafo.\n",
    "- `<a>`: Define un hipervínculo.\n",
    "- `<img>`: Define una imagen.\n",
    "- `<h1> a <h6>`: Define los encabezados.\n",
    "- `<ul>/<ol> y <li>`: Define listas desordenadas/ordenadas y elementos de lista.\n",
    "- `<table>, <tr>, <td>`: Define una tabla, fila de tabla y celda de tabla.\n",
    "\n",
    "Los atributos más comunes incluyen `class`, que define una o varias clases de estilo para un elemento, e `id`, que define un identificador único para un elemento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60bfb5a",
   "metadata": {},
   "source": [
    "### 2.C. Herramientas de inspección del navegador\n",
    "\n",
    "Cuando realizamos web scraping, es útil poder examinar la estructura HTML de la página web. Para ello, podemos utilizar las herramientas de inspección que ofrecen los navegadores modernos. Por ejemplo, en Chrome y Firefox, simplemente podemos hacer clic derecho en la página y seleccionar \"Inspeccionar\" o \"Inspeccionar elemento\". Esto nos mostrará el código HTML y nos permitirá explorar las diferentes etiquetas y sus atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb1701",
   "metadata": {},
   "source": [
    "## 3. Usando la librería requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5a81e",
   "metadata": {},
   "source": [
    "### 3.A. Instalación y primeros pasos\n",
    "\n",
    "Para instalar `requests` en tu entorno de Python, puedes utilizar `pip` o `conda`.\n",
    "\n",
    "Instalación de requests con pip:\n",
    "\n",
    "```python\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "Instalación de requests con conda:\n",
    "\n",
    "```python\n",
    "conda install -c anaconda requests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4daeac8",
   "metadata": {},
   "source": [
    "Una vez que `requests` esté instalado, puedes importarlo en tu script de Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b4da46",
   "metadata": {},
   "source": [
    "### 3.B. Realizando solicitudes HTTP (GET)\n",
    "\n",
    "La forma más común de solicitar datos es a través de una solicitud GET. En `requests`, esto se hace con el método `.get()`. Este método toma la URL a la que deseas hacer la solicitud como argumento.\n",
    "\n",
    "Como ejemplo, vamos a extraer información de la página web https://guiaempresas.universia.es/localidad/CAMARGO-CANTABRIA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de9dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://guiaempresas.universia.es/localidad/CAMARGO-CANTABRIA/'\n",
    "\n",
    "# Hacemos una solicitud GET a la URL y guardamos la respuesta en una variable\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58580ed",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos enviado una solicitud GET a la URL proporcionada y hemos guardado la respuesta en la variable `response`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9f27c",
   "metadata": {},
   "source": [
    "### 3.C. Explorando la respuesta (status code, content)\n",
    "\n",
    "Una vez que se ha realizado una solicitud, `requests` nos devuelve un objeto de respuesta que contiene el servidor de respuesta. Algunos de los atributos más útiles de este objeto son:\n",
    "\n",
    "- `response.status_code`: Este es el código de estado HTTP que se devolvió. Un código de estado de 200 significa que la solicitud fue exitosa.\n",
    "- `response.content` y `response.text`: Estos contienen el contenido de la respuesta. `.content` devuelve el contenido en bytes, mientras que `.text` lo convierte a una cadena de texto utilizando la codificación del texto de la respuesta.\n",
    "\n",
    "Aquí hay un ejemplo de cómo podríamos usar estos atributos para manejar la respuesta a nuestra solicitud GET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557db1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos que la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print('Solicitud exitosa!')\n",
    "else:\n",
    "    print('Hubo un error en la solicitud.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimimos el contenido de la respuesta\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ca604",
   "metadata": {},
   "source": [
    "## 4. Extracción de datos con Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b00d78",
   "metadata": {},
   "source": [
    "Beautiful Soup es una biblioteca de Python para extraer datos de archivos HTML y XML. Proporciona formas para buscar, navegar y modificar el árbol de análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac217c",
   "metadata": {},
   "source": [
    "### 4.A. Instalación y primeros pasos\n",
    "\n",
    "Para instalar Beautiful Soup en tu entorno de Python, puedes utilizar `pip` o `conda`.\n",
    "\n",
    "Instalación de requests con pip:\n",
    "\n",
    "```python\n",
    "pip install beautifulsoup4\n",
    "```\n",
    "\n",
    "Instalación de requests con conda:\n",
    "\n",
    "```python\n",
    "conda install -c anaconda beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d0145",
   "metadata": {},
   "source": [
    "Una vez instalado, puedes importarlo en tu script Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1650704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ca0fb",
   "metadata": {},
   "source": [
    "### 4.B. Parseando HTML con Beautiful Soup\n",
    "\n",
    "Beautiful Soup convierte los documentos HTML complejos en un árbol de objetos Python, como listas y diccionarios. Para parsear un documento HTML con Beautiful Soup, primero debemos crear un objeto `BeautifulSoup`.\n",
    "\n",
    "Por ejemplo, podemos usar el método `BeautifulSoup()` para parsear la respuesta HTML que obtuvimos en la sección anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60709c",
   "metadata": {},
   "source": [
    "En este ejemplo, `response.text` es el HTML que queremos parsear y `'html.parser'` es el analizador que queremos usar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8febd6a7",
   "metadata": {},
   "source": [
    "### 4.C. Buscar y extraer elementos de la página\n",
    "\n",
    "Una vez que hemos creado nuestro objeto `soup`, podemos usar sus métodos para buscar y extraer elementos del HTML.\n",
    "\n",
    "Por ejemplo, podemos usar el método `.find()` para buscar el primer elemento que coincida con el criterio que especificamos. O podemos usar el método `.find_all()` para buscar todos los elementos que coincidan.\n",
    "\n",
    "Podemos utilizar el inspector de elementos del navegador para identificar los elementos que contienen la información que buscamois. En este caso, observamos que las empresas que queremos extraer están dentro de una tabla con `class=\"ranking_einf\"`. Cada empresa está en un elemento `<tr>` de esta tabla. Dentro de cada elemento `<tr>`, hay 4 elementos `<td>` que contienen índice, nombre+url, localidad y provincia de la empresa, y que corresponden a las 4 columnas de la tabla.\n",
    "\n",
    "Aquí hay un ejemplo de cómo podríamos usar `.find()` para buscar la tabla y luego `.find_all()` para buscar todos los elementos `<tr>` dentro de la tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674114b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscamos la tabla con class=\"ranking_einf\" en el HTML\n",
    "table = soup.find('table', {'class': 'ranking_einf'})\n",
    "\n",
    "# Buscamos todos los elementos <tr> en la tabla\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Iteramos sobre los elementos <tr>\n",
    "for row in rows:\n",
    "    \n",
    "    # Buscamos todos los elementos <td> en el elemento <tr>\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    # Si hay elementos <td>, extrae e imprime la información de la empresa\n",
    "    if columns:\n",
    "        name = columns[1].text.strip()\n",
    "        url = columns[1].find('a')['href']\n",
    "        location = columns[2].text.strip()\n",
    "        province = columns[3].text.strip()\n",
    "        \n",
    "        print(f\"Nombre: {name}, Localidad: {location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc1404",
   "metadata": {},
   "source": [
    "Ahora que ya sabemos cómo extraer información de una página web utilizando `requests` y `Beautiful Soup`, vamos a dar un paso más allá. En lugar de simplemente imprimir la información, vamos a guardarla en un `DataFrame` de `pandas` para poder trabajar con ella de una manera más estructurada.\n",
    "\n",
    "`pandas` es una biblioteca de Python para la manipulación y análisis de datos. Un `DataFrame` es una estructura de datos bidimensional etiquetada en `pandas`, similar a una tabla de una base de datos SQL o una hoja de cálculo de Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Buscamos la tabla con class=\"ranking_einf\" en el HTML\n",
    "table = soup.find('table', {'class': 'ranking_einf'})\n",
    "\n",
    "# Buscamos todos los elementos <tr> en la tabla\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Creamos una lista vacía\n",
    "companies = []\n",
    "\n",
    "# Iteramos sobre los elementos <tr>\n",
    "for row in rows:\n",
    "    \n",
    "    # Buscamos todos los elementos <td> en el elemento <tr>\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    # Si hay elementos <td>, extrae e imprime la información de la empresa\n",
    "    if columns:\n",
    "        name = columns[1].text.strip()\n",
    "        url = 'https://guiaempresas.universia.es' + columns[1].find('a')['href']\n",
    "        location = columns[2].text.strip()\n",
    "        province = columns[3].text.strip()\n",
    "        \n",
    "        # Guardamos todos los datos de la empresa en una estructura de diccionario\n",
    "        company = {'Nombre': name, 'URL': url, 'Localidad': location, 'Provincia': province}\n",
    "        \n",
    "        # Añadimos los datos de la empresa a la lista\n",
    "        companies.append(company)\n",
    "        \n",
    "# Convertimos la lista de empresas a un dataframe\n",
    "df = pd.DataFrame.from_records(companies)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917dff24",
   "metadata": {},
   "source": [
    "Finalmente, visualizamos las primeras filas del `DataFrame` usando el método `head()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a07021",
   "metadata": {},
   "source": [
    "Esto nos mostrará una tabla con la información de todas las empresas que hemos extraído.\n",
    "\n",
    "Además, `pandas` nos proporciona una serie de métodos muy útiles para trabajar con nuestros datos, como ordenar el `DataFrame` por una columna, filtrar las filas que cumplen ciertas condiciones, calcular estadísticas, etc. Te animo a explorar la [documentación de pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) para aprender más sobre lo que puedes hacer con ella."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1cb2f",
   "metadata": {},
   "source": [
    "## 5. Ejercicio Práctico\n",
    "Ahora que ya hemos aprendido a extraer datos de una página web con `requests` y `Beautiful Soup`, y a guardar estos datos en un `DataFrame` de `pandas`, es hora de que practiquéis con un ejercicio.\n",
    "\n",
    "El objetivo de este ejercicio es crear un script que extraiga los datos de la ficha de una empresa en la web de la Guía de Empresas de Universia. Para esto, tendréis que hacer una nueva solicitud a la URL de la ficha de la empresa, que habéis guardado en vuestro `DataFrame`, y luego utilizar `Beautiful Soup` para extraer los datos de esta página.\n",
    "\n",
    "Por ejemplo, la URL de la primera empresa es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['URL'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735875a7",
   "metadata": {},
   "source": [
    "Aquí tenéis los pasos que debéis seguir:\n",
    "\n",
    "1. Elegir una empresa de vuestro `DataFrame`.\n",
    "2. Hacer una solicitud GET a la URL de la ficha de la empresa.\n",
    "3. Crear un objeto Beautiful Soup con el contenido de la respuesta.\n",
    "4. Identificar los elementos que contienen los datos que queréis extraer (podéis usar el inspector de elementos del navegador para esto).\n",
    "5. Utilizar los métodos de Beautiful Soup para extraer estos datos.\n",
    "6. Guardar los datos en un nuevo `DataFrame`.\n",
    "\n",
    "Podéis empezar con los siguientes datos:\n",
    "\n",
    "- Nombre de la empresa\n",
    "- Dirección\n",
    "- Número de teléfono\n",
    "- CNAE\n",
    "- Fecha de creación\n",
    "\n",
    "Por favor, recordad que el web scraping debe realizarse de manera responsable. Aseguraos de respetar los términos de servicio de la web y no sobrecargar el servidor con demasiadas solicitudes.\n",
    "\n",
    "Por último, os animo a explorar las posibilidades de `pandas` para analizar los datos que habéis recogido. Por ejemplo, podríais ordenar las empresas por fecha de fundación, filtrarlas por actividad, etc.\n",
    "\n",
    "¡Buena suerte!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace7b6a",
   "metadata": {},
   "source": [
    "### 5.A. Tareas Opcionales\n",
    "Si terminas el ejercicio principal y te sientes con ganas de un desafío adicional, aquí tienes algunas tareas opcionales que puedes intentar:\n",
    "\n",
    "#### Tarea O1: \n",
    "Descarga los datos de todas las 30 empresas que aparecen en la primera página del listado en la página de Guía Empresas. Esto te dará más datos para trabajar y te permitirá practicar la escritura de código para automatizar el proceso de navegación y extracción de datos en varias páginas. Guarda los resultados en un dataframe de pandas y escribe el dataframe en un archivo CSV o XLSX. De este modo, podrás utilizar los datos para futuros análisis y no tendrás que volver a hacer web scraping cada vez que quieras trabajar con ellos.\n",
    "\n",
    "#### Tarea O2: \n",
    "Realiza una visualización de los datos que has extraído. Por ejemplo, podrías hacer un histograma de las fechas de creación de las empresas para ver cómo ha cambiado el número de nuevas empresas con el tiempo. También podrías imprimir en pantalla una lista de los CNAEs de las empresas, con el número de empresas que tienen ese CNAE entre paréntesis. Esto te permitirá ver cuáles son los CNAEs más comunes entre las empresas de la lista."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10df96",
   "metadata": {},
   "source": [
    "## 6. Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4e0bd",
   "metadata": {},
   "source": [
    "### 6.A. ¿Qué es Selenium?\n",
    "\n",
    "[Selenium](https://www.selenium.dev/) es una herramienta muy poderosa en el mundo del web scraping. A diferencia de `requests` y `Beautiful Soup`, que son herramientas para hacer solicitudes HTTP y analizar el contenido HTML de las respuestas, Selenium es una herramienta para automatizar navegadores web.\n",
    "\n",
    "Esto significa que con Selenium puedes hacer cosas como hacer clic en botones, rellenar formularios, desplazarte por una página, esperar a que se carguen elementos específicos, etc. En otras palabras, Selenium puede interactuar con una página web de la misma manera que un usuario humano.\n",
    "\n",
    "Además, como Selenium controla un navegador web real, puede manejar JavaScript. Esto es muy útil, ya que muchas páginas web modernas utilizan JavaScript para cargar o mostrar contenido. Una vez que Selenium ha cargado el contenido de la página web, puedes utilizar Beautiful Soup para analizar el HTML y extraer los datos.\n",
    "\n",
    "### 6.B. ¿Cuándo usar Selenium?\n",
    "Dado que Selenium es mucho más poderoso que `requests` y `Beautiful Soup`, podrías pensar que siempre es mejor usar Selenium para el web scraping. Sin embargo, esta potencia viene con un coste.\n",
    "\n",
    "Primero, Selenium es mucho más lento que `requests` y `Beautiful Soup`. Esto se debe a que Selenium tiene que cargar toda la página web en un navegador real, incluyendo todas las imágenes, CSS, JavaScript, etc., mientras que `requests` solo descarga el HTML de la página.\n",
    "\n",
    "Segundo, Selenium es mucho más complicado de usar que `requests` y `Beautiful Soup`. Con Selenium tienes que lidiar con cosas como tiempos de espera, excepciones, manejo de ventanas y pestañas, etc.\n",
    "\n",
    "Por lo tanto, en general, es mejor usar `requests` y `Beautiful Soup` siempre que sea posible, y recurrir a Selenium solo cuando sea necesario. Algunas situaciones en las que podría ser necesario usar Selenium incluyen:\n",
    "\n",
    "- La página web utiliza JavaScript para cargar o mostrar el contenido que quieres extraer.\n",
    "- Necesitas interactuar con la página web, por ejemplo, haciendo clic en botones o rellenando formularios.\n",
    "- La página web requiere que inicies sesión y la autenticación es complicada con `requests`.\n",
    "\n",
    "Aunque no vamos a hacer ninguna demostración práctica de Selenium en esta clase, os animo a explorar esta herramienta por vuestra cuenta. Selenium puede ser una adición muy valiosa a vuestro conjunto de herramientas de web scraping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d7834",
   "metadata": {},
   "source": [
    "## 7. Recapitulación y Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc2461",
   "metadata": {},
   "source": [
    "### 7.A. Resumen de lo aprendido\n",
    "\n",
    "En esta clase hemos cubierto una serie de conceptos y técnicas clave para el web scraping con Python. Aquí hay un breve resumen de lo que hemos aprendido:\n",
    "\n",
    "1. **Conceptos de HTML**: Hemos repasado los fundamentos del HTML, que es esencial para entender cómo están estructuradas las páginas web y cómo podemos extraer datos de ellas.\n",
    "\n",
    "2. **Uso de la librería requests**: Hemos aprendido a hacer solicitudes HTTP a una página web utilizando la librería requests de Python. Hemos visto cómo manejar las respuestas de las solicitudes y cómo extraer el contenido de una página.\n",
    "\n",
    "3. **Extracción de datos con Beautiful Soup**: Hemos utilizado Beautiful Soup para parsear el HTML de una página web y buscar elementos específicos en ella. Hemos visto cómo extraer información de estos elementos y almacenarlos en una estructura de datos.\n",
    "\n",
    "4. **Ejercicio Práctico**: Hemos puesto en práctica todo lo aprendido realizando un ejercicio de web scraping en la página de Guía Empresas, en el que hemos extraído información detallada de las empresas.\n",
    "\n",
    "5. **Uso de Selenium**: Hemos discutido brevemente el uso de Selenium, una herramienta que nos permite interactuar con páginas web de una manera más avanzada, como si estuviéramos utilizando un navegador web.\n",
    "\n",
    "### 7.B. Tiempo para preguntas\n",
    "\n",
    "Ahora es tu momento para preguntar cualquier duda que tengas sobre lo que hemos cubierto en la clase. No dudes en hacer cualquier pregunta, por pequeña que sea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015132e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
